{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"C43ua3eJMx7X"},"outputs":[],"source":["# remove these lines if not running on notebooks\n","#%matplotlib notebook\n","run_from_notebook = False"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kBdU9skYM90f"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"beACEMm_Mx7c"},"source":["## Import the required packages\n","Insert here all the packages you require, so in case they are not found an error will be shown before any other operation is performed."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5zZIFYFbMx7e"},"outputs":[],"source":[" # import the required packages\n","import os\n","from os.path import exists\n","import time\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision\n","import torchvision.transforms as transforms\n","import pandas as pd\n","import csv\n","import glob\n","from zipfile import ZipFile\n","from torch.utils.data import Dataset, DataLoader\n","from torchsampler import ImbalancedDatasetSampler #pip install torchsampler\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import train_test_split\n","import math\n","from sklearn.metrics import f1_score\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UAyZkw5IMx7f"},"outputs":[],"source":["path_to_file = 'MachineLearningCVE'\n","file_exists = exists(path_to_file)\n","\n","if file_exists :\n","    print(f'The file {path_to_file} exists')\n","\n","else :\n","   with ZipFile('/content/drive/MyDrive/IDS_CNN/MachineLearningCSV.zip', 'r') as zipObj:\n","      \n","      zipObj.extractall()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FIIqS8qZTW6s"},"outputs":[],"source":["gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Not connected to a GPU')\n","else:\n","  print(gpu_info)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V1G82GuO-tez"},"outputs":[],"source":["from psutil import virtual_memory\n","ram_gb = virtual_memory().total / 1e9\n","print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n","\n","if ram_gb < 20:\n","  print('Not using a high-RAM runtime')\n","else:\n","  print('You are using a high-RAM runtime!')"]},{"cell_type":"markdown","metadata":{"id":"jQEU0f4aMx7h"},"source":["## Set hyperparameters and options\n","Set here your hyperparameters (to be used later in the code), so that you can run and compare different experiments operating on these values. \n","<br>_Note: a better alternative would be to use command-line arguments to set hyperparameters and other options (see argparse Python package)_"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6BWzxAi6Mx7h"},"outputs":[],"source":["# hyperparameters\n","batch_size = 32\n","learning_rate = 0.001\n","epochs = 100\n","momentum = 0.1\n","lr_step_size = 1000   # if < epochs, we are using decaying learning rate\n","lr_gamma = 0.1\n","data_augmentation = True\n","dropout = 0.1\n","activation = nn.ReLU()\n","block_dim = 10\n","input_dim = 78*block_dim\n","num_classes = 2\n","\n","# make visible only one GPU at the time\n","os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # <-- should be the ID of the GPU you want to use\n","\n","# options\n","device = \"cuda:0\"           # put here \"cuda:0\" if you want to run on GPU\n","monitor_display = True      # whether to display monitored performance plots\n","display_first_n = 0         # how many samples/batches are displayed\n","num_workers = 4             # how many workers (=threads) for fetching data\n","pretrained = False          # whether to test a pretrained model (to be loaded) or train a new one\n","display_errors = True       # whether to display errors (only in pretrained mode)"]},{"cell_type":"markdown","metadata":{"id":"WiurlRYnMx7i"},"source":["## Define the model architecture\n","Define here your network.\n","<br>_Note: a better alternative would be to have a pool of network architectures defined in a python file (module) that one could import_"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fZ34Q9e1o6O5"},"outputs":[],"source":["class IDSNet(nn.Module):\n","    def __init__(self,block_dim,num_classes,device):\n","        super(IDSNet, self).__init__()\n","        # kernel\n","        self.block_dim = block_dim\n","        self.num_classes = num_classes\n","\n","        conv_layers = []\n","        conv_layers.append(nn.Conv1d(in_channels=78,out_channels=64,kernel_size=3,padding=1)) # ;input_dim,64\n","        conv_layers.append(nn.BatchNorm1d(64))\n","        conv_layers.append(nn.ReLU(True))\n","\n","        conv_layers.append(nn.Conv1d(in_channels=64,out_channels=128,kernel_size=3,padding=1)) #(input_dim,128)\n","        conv_layers.append(nn.BatchNorm1d(128))\n","        conv_layers.append(nn.ReLU(True))\n","\n","        self.conv = nn.Sequential(*conv_layers).to(device)\n","\n","        fc_layers = []\n","        fc_layers.append(nn.Linear(block_dim*128,num_classes))\n","        self.classifier = nn.Sequential(*fc_layers).to(device)\n","\n","    def forward(self, x):\n","        x = self.conv(x)\n","        x = torch.flatten(x,1)\n","        x = self.classifier(x)\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"enkZt9hKoLY3"},"outputs":[],"source":["class IDSNet2(nn.Module):\n","    def __init__(self,block_dim,num_classes,device):\n","        super(IDSNet2, self).__init__()\n","        # kernel\n","        self.block_dim = block_dim\n","        self.num_classes = num_classes\n","        \n","        \n","        self.layers = nn.Sequential(\n","            nn.Linear(block_dim*78, num_classes),\n","\n","        )\n","\n","    def forward(self, x):\n","      x = torch.flatten(x,1)\n","      x = self.layers(x)\n","      return x"]},{"cell_type":"markdown","metadata":{"id":"6m6M2kQVMx7k"},"source":["## Create the building blocks for training\n","Create an instance of the network, the loss function, the optimizer, and learning rate scheduler."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q8HmL3BYMx7m"},"outputs":[],"source":["net = IDSNet(block_dim=block_dim,num_classes= num_classes, device=device)\n","\n","\n","# create loss function\n","criterion = nn.CrossEntropyLoss()\n","\n","# create Adam optimizer\n","optimizer = optim.Adam(net.parameters(),lr=learning_rate)\n","\n","# create learning rate scheduler\n","scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=lr_step_size, gamma=lr_gamma)\n","\n","# experiment ID\n","experiment_ID = \"%s_%s_%s_bs(%d)lr(%.4f_%d_%.1f)m(%.1f)e(%d)act(%s)xavier(yes)da(%s)do(%.1f)BN\" % (type(net).__name__, type(criterion).__name__, type(optimizer).__name__,\n","                batch_size, learning_rate, lr_step_size, lr_gamma, momentum, epochs, type(activation).__name__, data_augmentation, dropout)"]},{"cell_type":"markdown","metadata":{"id":"uCHOL6iAMx7m"},"source":["## Load csv data\n","Load data in multiple CSV file "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2UciRRIDMx7n"},"outputs":[],"source":["#read in all csv file\n","\n","base_dir = \"MachineLearningCVE\"\n","\n","\n","all_files = glob.glob(os.path.join(base_dir, \"*.csv\"))\n","\n","data = pd.concat((pd.read_csv(f) for f in all_files), ignore_index = True)\n","\n","\n","print(\"Finished reading in {} entires\".format(str(data.shape[0])))\n","\n"]},{"cell_type":"markdown","metadata":{"id":"G5wY7kcSMx7n"},"source":["## Cleaning Data\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9LkTrc0lMx7o"},"outputs":[],"source":["data.rename(columns=lambda x: x.lower().lstrip()\n","          .rstrip().replace(\" \", \"_\"), inplace=True)\n","data.columns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eF8B3GnHMx7o"},"outputs":[],"source":["data.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4YuwrTFsMx7p"},"outputs":[],"source":["#Checking for NULL values:\n","\n","print('Null values in the dataset are: ',len(data[data.isnull().any(1)]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y_2hAi4OMx7p"},"outputs":[],"source":["data.replace([np.inf, -np.inf], np.nan, inplace=True)\n","data.dropna(inplace=True)\n","data.reset_index(drop=True, inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dsJ2LOegMx7p"},"outputs":[],"source":["data.head()"]},{"cell_type":"markdown","metadata":{"id":"6Fbw_b9CMx7q"},"source":["## Conversion of Classification\n","These blocks convert Multiclass classification in binary classification"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E1JXP4XzMx7q"},"outputs":[],"source":["label_binary = []\n","for i in data['label'].values:\n","    if i=='BENIGN':\n","        label_binary.append(1)\n","    else:\n","        label_binary.append(0)\n","print(len(label_binary))\n","print(label_binary[:10])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ij20GG7kMx7r"},"outputs":[],"source":["data['label_binary'] = label_binary\n","data.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_NY68e_sMx7r"},"outputs":[],"source":["data.drop('label', axis=1, inplace=True)\n","print('Shape of the data is:')\n","print(data.shape)\n","print('='*80)\n","print('Features of the dataset:')\n","print(data.columns)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lPLP9u2pMx7r"},"outputs":[],"source":["data.head()"]},{"cell_type":"markdown","metadata":{"id":"gW0213TqMx7r"},"source":["## Visualizes the data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hpm5L0MuMx7s"},"outputs":[],"source":["data[\"label_binary\"].value_counts()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nAYkRZrUMx7s"},"outputs":[],"source":["data[\"label_binary\"].value_counts(normalize=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CxtgSC52Mx7s"},"outputs":[],"source":["\n","data[data[\"label_binary\"] != \"1\"][\"label_binary\"].value_counts().plot(kind='bar')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1ox_kFGoMx7s"},"outputs":[],"source":["path_to_file = 'dataset.csv'\n","file_exists = exists(path_to_file)\n","\n","if file_exists :\n","    print(f'The file {path_to_file} exists')\n","\n","else :\n","    data.to_csv('dataset.csv', index=False)"]},{"cell_type":"markdown","metadata":{"id":"1QPNW5o4Mx7t"},"source":["## Dataset Generator"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nsxchX7fMx7t"},"outputs":[],"source":["class CsvDataset(Dataset) :\n","    def __init__(self, file_name, block_dim, transform = None) :\n","\n","        #read csv file and load row data into variables\n","        file_out = pd.read_csv(file_name)\n","        self.x = file_out.iloc[1:, 0:78].values\n","        self.y = file_out.iloc[1:, 78].values\n","\n","        self.block_dim = block_dim\n","        \n","        \n","        self.num_sample = math.floor(len(self.x)/block_dim)\n","\n","        self.data_sample = []\n","        \n","        for i in range(self.num_sample):\n","            sample = self.x[i*block_dim]\n","            \n","\n","            for j in range(i*block_dim, (i*block_dim)+self.block_dim-1) :\n","                sample = np.concatenate((sample, self.x[j]), axis=0)\n","            \n","            self.data_sample.append(sample)     \n","\n","\n","        self.transform = transform\n","        \n","        self.targets = [] \n","\n","        for i in range(self.num_sample) :\n","            target = 1\n","\n","            for j in range(i*block_dim, (i*block_dim)+block_dim-1) :\n","                if self.y[j] == 0 :\n","                    target = 0\n","                    break\n","            \n","            self.targets.append(target)\n","\n","        self.targets = torch.tensor(self.targets)\n","            \n","\n","    def __len__(self) :\n","        return self.num_sample\n","\n","    def __getitem__(self, index) :\n","        sample = self.data_sample[index]\n","        sample = np.reshape(sample, (-1, block_dim))\n","\n","        if self.transform:\n","            sample = self.transform(sample)\n","\n","        target = self.targets[index]\n","\n","        return sample, target"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hMXSK9B1ImZ9"},"outputs":[],"source":["path1_to_file = 'test_dataset.csv'\n","file1_exists = exists(path1_to_file)\n","\n","path2_to_file = 'x_test.csv'\n","file2_exists = exists(path2_to_file)\n","\n","path3_to_file = 'y_test.csv'\n","file3_exists = exists(path3_to_file)\n","\n","if file1_exists:\n","  if file2_exists and file3_exists:\n","    print(f'The files {path2_to_file}, {path3_to_file}  exist')\n","  else :\n","    file_out = pd.read_csv(\"test_dataset.csv\")\n","    x = file_out.iloc[1:, 0:78].values\n","    y = file_out.iloc[1:, 78].values\n","    np.savetxt(\"x_test.csv\", x, delimiter=\",\")\n","    np.savetxt(\"y_test.csv\", y, delimiter=\",\")\n","\n","else:\n","  print('Execute only for debug')\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tfBJpnOXImZ-"},"outputs":[],"source":["path1_to_file = 'train_dataset.csv'\n","file1_exists = exists(path1_to_file)\n","\n","path2_to_file = 'x_train.csv'\n","file2_exists = exists(path2_to_file)\n","\n","path3_to_file = 'y_train.csv'\n","file3_exists = exists(path3_to_file)\n","\n","if file1_exists:\n","  if file2_exists and file3_exists:\n","    print(f'The files {path2_to_file}, {path3_to_file}  exist')\n","  else :\n","    file_out = pd.read_csv(\"train_dataset.csv\")\n","    x = file_out.iloc[1:, 0:78].values\n","    y = file_out.iloc[1:, 78].values\n","    np.savetxt(\"x_train.csv\", x, delimiter=\",\")\n","    np.savetxt(\"y_train.csv\", y, delimiter=\",\")\n","\n","else:\n","  print('Execute only for debug')"]},{"cell_type":"markdown","metadata":{"id":"wKx8q6msMx7t"},"source":["## Create Datasets\n","\n","This includes training/validation split, where possible. In our example, MNIST does not have a validation set, so we use the test set as validation set (warning: see comments in the code).\n","Note: in general, you might need to implement your own Dataset in a separate Python file, and then import it in this file in order to create the dataset. The training/validation/test data split is also on your own, you may consider to embed it in your Dataset class"]},{"cell_type":"code","source":["train = data.iloc[1:1979514, :]\n","test = data.iloc[1979514:, :]"],"metadata":{"id":"z-aHvOrGunM8"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9HNTvduOMx7u"},"outputs":[],"source":["path1_to_file = 'train_dataset.csv'\n","file1_exists = exists(path1_to_file)\n","\n","path2_to_file = 'test_dataset.csv'\n","file2_exists = exists(path2_to_file)\n","\n","\n","if file1_exists and file2_exists:\n","    print(f'The files {path1_to_file}, {path2_to_file}  exist')\n","\n","else :\n","  train = data.iloc[1:1979514, :]\n","  test = data.iloc[1979514:, :]\n","  train.to_csv('train_dataset.csv', index=False)\n","  test.to_csv('test_dataset.csv', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vD0lW49EMx7u"},"outputs":[],"source":["dataset_train = CsvDataset(file_name='train_dataset.csv', block_dim = block_dim)\n","dataset_valid = CsvDataset(file_name='test_dataset.csv', block_dim = block_dim)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O8lr4Zhx6J34"},"outputs":[],"source":["train[\"label_binary\"].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1gftm2e1ImZ_"},"outputs":[],"source":["train[\"label_binary\"].value_counts(normalize=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yI6HRVPqImZ_"},"outputs":[],"source":["train[train[\"label_binary\"] != \"1\"][\"label_binary\"].value_counts().plot(kind='bar')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tIiNNs5rImaA"},"outputs":[],"source":["test[\"label_binary\"].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JGRjHxMoImaA"},"outputs":[],"source":["test[\"label_binary\"].value_counts(normalize=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IMqTO-KZImaA"},"outputs":[],"source":["test[test[\"label_binary\"] != \"1\"][\"label_binary\"].value_counts().plot(kind='bar')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"peMZxJ4CE2ln"},"outputs":[],"source":["sample,target = dataset_train[1]\n","\n","print(len(sample))\n","print(sample)\n","sample.shape\n","print(target)"]},{"cell_type":"markdown","metadata":{"id":"rBvUc_M2Mx7u"},"source":["## Data Scaler\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P0bD0qtmMx7v"},"outputs":[],"source":["# Standardizing\n","def apply_scaler(X, scaler):\n","    X_tmp = []\n","    for x in X:\n","        x_shape = x.shape\n","        X_tmp.append(scaler.transform(x.flatten()[:,np.newaxis]).reshape(x_shape))\n","    X_tmp = np.array(X_tmp)\n","    return X_tmp\n","\n","scaler = StandardScaler()\n","scaler.fit(np.vstack(dataset_train.data_sample).flatten()[:,np.newaxis].astype(float))\n","dataset_train.data_sample = apply_scaler(dataset_train.data_sample, scaler)\n","dataset_valid.data_sample = apply_scaler(dataset_valid.data_sample, scaler)"]},{"cell_type":"markdown","metadata":{"id":"1bV_iClxMx7v"},"source":["## Define data transforms\n","Data transforms are applied sample-wise at _batch generation_ time: they are _not_ applied until you use a Dataloader and fetch data from it. In general, they serve to transform your data into what the neural network expects. Data should be _at least_ converted to tensors whose shape corresponds to network input, and possibly normalized so as to be 0-centered roughly in the [-1,1] range. \n","\n","In this example, we also apply a transform to the targets (labels), so as to have one-hot tensor that can be compared with network outputs using the loss function.\n","\n","Optionally, we may also apply data augmentation (on the training set, only)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nXL9btqsMx7v"},"outputs":[],"source":["class Convert(object):\n","    def __call__(self, sample):\n","      return torch.from_numpy(sample).float()\n","\n","# define data transform for train\n","transform_train = Convert()\n","\n","# define data transform for validation\n","transform_valid = Convert()\n","\n","# set data and target transforms on both datasets\n","dataset_train.transform = transform_train\n","dataset_valid.transform = transform_valid"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w9zmOb2VRx6T"},"outputs":[],"source":["sample, target = dataset_train[1]\n","\n","print(sample.size())\n","print(sample)\n","print(target)"]},{"cell_type":"markdown","metadata":{"id":"tQ7ggD9GMx7w"},"source":["## Create data loaders\n","Dataloaders are in-built PyTorch objects that serve to sample batches from datasets. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cUyojjasMx7w"},"outputs":[],"source":["# create data loaders\n","# NOTE 1: shuffle helps training\n","# NOTE 2: in test mode, batch size can be as high as the GPU can handle (faster, but requires more GPU RAM)\n","\n","dataloader_train = torch.utils.data.DataLoader(\n","    dataset_train,\n","    sampler=ImbalancedDatasetSampler(dataset_train, labels=dataset_train.targets),\n","    batch_size=batch_size,\n","    num_workers=num_workers,\n","    pin_memory=True\n",")\n","\n","dataloader_valid = torch.utils.data.DataLoader(\n","    dataset_valid, \n","    batch_size=batch_size, \n","    num_workers=num_workers, \n","    pin_memory=True\n",") "]},{"cell_type":"markdown","metadata":{"id":"kUlBBdGuMx7w"},"source":["## Define train function\n","It is preferable (but not mandatory) to embed training (1 epoch) code into a function, and call that function later during the training phase, at each epoch."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eI6oG3nYMx7w"},"outputs":[],"source":["# define train function (1 epoch)\n","# returns average loss and accuracy\n","def train(dataset, dataloader):\n","\n","    # switch to train mode\n","    net.train()\n","\n","    # reset performance measures\n","    loss_sum = 0.0\n","    #correct = 0\n","\n","    train_targets = []\n","    train_outputs = []\n","\n","\n","    # 1 epoch = 1 complete loop over the dataset\n","    for batch in dataloader:\n","\n","        # get data from dataloader\n","        inputs, targets = batch\n","\n","        # move data to device\n","        inputs, targets = inputs.to(device, non_blocking=True), targets.to(device, non_blocking=True)\n","\n","        # zero the parameter gradients\n","        optimizer.zero_grad()\n","\n","        # forward pass\n","        outputs = net(inputs)\n","\n","        # calculate loss\n","        loss = criterion(outputs, targets)\n","\n","        # loss gradient backpropagation\n","        loss.backward()\n","\n","        # net parameters update\n","        optimizer.step()\n","\n","        # accumulate loss\n","        loss_sum += loss.item()\n","\n","       \n","\n","    \n","\n","        # accumulate correct outputs (for accuracy calculation)\n","        outputs_max = torch.argmax(outputs, dim=1)\n","        targets_max = targets #torch.argmax(targets, dim=1)\n","        #correct += outputs_max.eq(targets_max).sum().float().detach().cpu().numpy()\n","\n","        train_outputs.append(outputs_max)\n","        train_targets.append(targets_max)\n","\n","        #print(train_outputs)\n","        #print(train_targets)\n","\n","    # step learning rate scheduler\n","    scheduler.step()\n","\n","   \n","\n","    train_outputs = torch.cat(train_outputs)\n","    train_targets = torch.cat(train_targets)\n","\n","    train_outputs = train_outputs.detach().cpu().numpy()\n","    train_targets = train_targets.detach().cpu().numpy()\n","\n","    # return average loss and accuracy\n","    return loss_sum / len(dataloader), f1_score(train_outputs, train_targets, average='binary')"]},{"cell_type":"markdown","metadata":{"id":"EkrdW1VVMx7x"},"source":["## Define test function\n","It is preferable (but not mandatory) to embed the test code into a function, and call that function whenever needed. For instance, during training for validation at each epoch, or after training for testing, or for deploying the model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tpklLXUoMx7x"},"outputs":[],"source":["# define test function\n","# returns predictions\n","def test(dataset, dataloader):\n","\n","    # switch to test mode\n","    net.eval()  \n","\n","    # initialize predictions\n","    predictions = torch.zeros(len(dataset), dtype=torch.int64)\n","    sample_counter = 0\n","\n","    # do not accumulate gradients (faster)\n","    with torch.no_grad():\n","\n","        # test all batches\n","        for batch in dataloader:\n","\n","            # get data from dataloader [ignore labels/targets as they are not used in test mode]\n","            inputs = batch[0]\n","\n","            # move data to device\n","            inputs = inputs.to(device, non_blocking=True)\n","\n","            # forward pass\n","            outputs = net(inputs)\n","\n","            # store predictions\n","            outputs_max = torch.argmax(outputs, dim=1)\n","            for output in outputs_max:\n","                predictions[sample_counter] = output\n","                sample_counter += 1\n","\n","    return predictions"]},{"cell_type":"markdown","metadata":{"id":"n9aaW8yJMx7x"},"source":["## Train a new model or test a pretrained one\n","The code below also includes visual loss/accuracy monitoring during training, both on training and validation sets. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Miobj8GUMx7x"},"outputs":[],"source":["# pretrained model not available --> TRAIN a new one and save it\n","if not pretrained:\n","    \n","    # reset performance monitors\n","    losses = []\n","    train_accuracies = []\n","    valid_accuracies = []\n","    ticks = []\n","    \n","    # move net to device\n","    net.to(device)\n","    \n","    # start training\n","    for epoch in range(1, epochs+1):\n","\n","        # measure time elapsed\n","        t0 = time.time()\n","        \n","        # train\n","        avg_loss, accuracy_train = train(dataset_train, dataloader_train)\n","\n","        # test on validation\n","        predictions = test(dataset_valid, dataloader_valid)\n","        #accuracy_valid = 100. * predictions.eq(dataset_valid.targets).sum().float() / len(dataset_valid)\n","        accuracy_valid = f1_score(predictions,dataset_valid.targets, average='binary')            \n","        # update performance history\n","        losses.append(avg_loss)\n","        train_accuracies.append(accuracy_train)\n","        valid_accuracies.append(accuracy_valid)\n","        ticks.append(epoch)\n","            \n","\n","\n","        # print or display performance\n","        if not monitor_display:    \n","            print (\"\\nEpoch %d\\n\"\n","                \"...TIME: %.1f seconds\\n\"\n","                \"...loss: %g (best %g at epoch %d)\\n\"\n","                \"...training accuracy: %.2f%% (best %.2f%% at epoch %d)\\n\"\n","                \"...validation accuracy: %.2f%% (best %.2f%% at epoch %d)\" % (\n","                epoch,\n","                time.time()-t0,\n","                avg_loss, min(losses), ticks[np.argmin(losses)],\n","                accuracy_train, max(train_accuracies), ticks[np.argmax(train_accuracies)],\n","                accuracy_valid, max(valid_accuracies), ticks[np.argmax(valid_accuracies)]))\n","            \n","        else:\n","            fig, ax1 = plt.subplots(figsize=(12, 8), num=1)\n","            ax1.set_xticks(np.arange(0, epochs+1, step=epochs/10.0))\n","            ax1.set_xlabel('Epochs')\n","            ax1.set_ylabel(type(criterion).__name__, color='blue')\n","            ax1.set_ylim(0.0001, 1)\n","            ax1.tick_params(axis='y', labelcolor='blue')\n","            ax1.set_yscale('log')\n","            ax1.plot(ticks, losses, 'b-', linewidth=1.0, aa=True, \n","                label='Training (best at ep. %d)' % ticks[np.argmin(losses)])\n","            ax1.legend(loc=\"lower left\")\n","            ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n","            ax2.set_ylabel('F1Score', color='red')\n","            ax2.set_ylim(0.5, 1)\n","            ax2.set_yticks(np.arange(0.5, 1, step=0.02))\n","            ax2.tick_params(axis='y', labelcolor='red')\n","            ax2.plot(ticks, train_accuracies, 'r-', linewidth=1.0, aa=True, \n","                label='Training (%.2f, best %.2f at ep. %d)' % (accuracy_train, max(train_accuracies), ticks[np.argmax(train_accuracies)]))\n","            ax2.plot(ticks, valid_accuracies, 'r--', linewidth=1.0, aa=True, \n","                label='Validation (%.2f, best %.2f at ep. %d)' % (accuracy_valid, max(valid_accuracies), ticks[np.argmax(valid_accuracies)]))\n","            ax2.legend(loc=\"lower right\")\n","            plt.xlim(0, epochs+1)\n","            # this works if running from notebooks\n","            if run_from_notebook:\n","                fig.show()\n","                fig.canvas.draw()\n","            # this works if running from console\n","            else:\n","                plt.draw()\n","                #plt.pause(0.001)\n","                plt.show()\n","           # plt.savefig(experiment_ID + \".png\", dpi=300)\n","            fig.clear()\n","\n","        # save model if validation performance has improved\n","        if (epoch-1) == np.argmax(valid_accuracies):\n","            torch.save({\n","                'net': net,\n","                'accuracy': max(valid_accuracies),\n","                'epoch': epoch\n","            }, experiment_ID + \".tar\")\n","\n","# pretrained model available -> load it and test\n","else:\n","\n","    # load pretrained model\n","    checkpoint = torch.load(experiment_ID + \".tar\", map_location=lambda storage, loc: storage)\n","    net = checkpoint['net']\n","    print (\"Loaded pretrained model\\n...trained for %d epochs\\n...reached accuracy %.2f\" % (checkpoint['epoch'], checkpoint['accuracy']))\n","\n","    # move net to device\n","    net.to(device)\n","\n","    # test\n","    predictions = test(dataset_valid, dataloader_valid)\n","    accuracy = 100. * predictions.eq(dataset_valid.targets).sum().float() / len(dataset_valid)\n","    print (\"Accuracy on test set is %.2f\" % accuracy)\n","\n","    # display errors\n","    if display_errors:\n","\n","        # predictions / target comparisons = 1 for match, 0 for mismatch\n","        # we subtract 1, so we have 0 for match, -1 for mismatch\n","        # nonzero elements are thus all mismatches\n","        errors = torch.nonzero(~predictions.eq(dataset_valid.targets))\n","\n","        # get errors samples and convert them to torch tensors\n","        error_samples = torch.zeros(len(errors), 1, 28, 28)\n","        conversion = Convert()\n","        for i, e in enumerate(errors):\n","            error_samples[i] = conversion(dataset_valid.data[e.item()])\n","\n","        # make a grid of images and show\n","        img = torchvision.utils.make_grid(error_samples, nrow=20)\n","        img = img/255       # move data to [0,1] since pyplot expects float images to be in [0,1]\n","        npimg = img.numpy() # convert to numpy, since pyplot expects numpy images\n","        plt.imshow(np.transpose(npimg, (1, 2, 0)))  # CHW to WHC reshape\n","        plt.title('Errors')\n","        plt.show()"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3.10.5 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.5"},"vscode":{"interpreter":{"hash":"1215b1aecdd8c83375b241d0c862057777d1b41ed412da38189195d086960e71"}}},"nbformat":4,"nbformat_minor":0}